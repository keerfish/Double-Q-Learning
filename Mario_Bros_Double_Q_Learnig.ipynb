{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12892abd-3076-4bd1-acda-33c03cc7237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "from skimage import transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44bde1-daf4-412e-8cc5-f1625b429f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The game has 8 worlds and 4 stages.\n",
    "The quality of the game rendering from 0 to 4 decreases.\n",
    "The list of actions can find from this link https://github.com/Kautenja/gym-super-mario-bros/blob/master/gym_super_mario_bros/actions.py\n",
    "\"\"\"\n",
    "\n",
    "WORLD = 1  # min:1, max:8\n",
    "STAGE = 1  # min:1, max:4\n",
    "LEVEL = f\"{WORLD}-{STAGE}\"\n",
    "QUALITY = 0 # min:0, max:3\n",
    "MY_ACTIONS = [[\"right\"], [\"right\", \"A\"]] # MY_ACTIONS = [[\"right\"], [\"right\", \"A\"],[\"right\", \"B\"],[\"left\"],[\"left\", \"A\"]]\n",
    "\n",
    "episodes=200000 # suggest running the loop for at least 40,000 episodes\n",
    "\n",
    "DEFAULT_GAME = f\"SuperMarioBros-{LEVEL}-v{QUALITY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d9044-b404-403b-9eab-844a2e3b9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make(DEFAULT_GAME)\n",
    "env = JoypadSpace(env, MY_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b002ad-335a-44f7-becd-dc9297ea9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"\n",
    "    mini cnn structure\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == 'online':\n",
    "            return self.online(input)\n",
    "        elif model == 'target':\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649a401-3f0e-497c-9620-c69d7ec6c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir, checkpoint=None, world=WORLD, stage=STAGE):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.world = world\n",
    "        self.stage = stage\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.gamma = 0.9\n",
    "\n",
    "        self.curr_step = 0\n",
    "        self.burnin = 1e5  \n",
    "        self.learn_every = 3   \n",
    "        self.sync_every = 1e4   \n",
    "\n",
    "        self.save_every = 5e5   \n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device='cuda')\n",
    "        if checkpoint:\n",
    "            self.load(checkpoint)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, choose an epsilon-greedy action and update value of step.\n",
    "        \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model='online')\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "        reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "        done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "        self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model='online')\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{self.world}_{self.stage}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "\n",
    "\n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4433e5d-920c-4f6b-afae-446a8e74c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ead497-a2ef-4d95-988b-72fcc04ee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        resize_obs = transform.resize(observation, self.shape)\n",
    "        # cast float back to uint8\n",
    "        resize_obs *= 255\n",
    "        resize_obs = resize_obs.astype(np.uint8)\n",
    "        return resize_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ad199-333c-4e43-aa74-096fde363433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535f4f98-af56-4727-84be-718f8d1e54cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "checkpoint = None \n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "# wrap the env in the record video\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"./Super_Mario\", name_prefix=\"test-video\", episode_trigger=lambda x: x % 2 == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f6d64-fb99-46df-9451-4030d238fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the recorder\n",
    "env.start_video_recorder()\n",
    "\n",
    "for e in range(episodes):\n",
    "    \n",
    "     state = env.reset()\n",
    "    \n",
    "        # Play the game!\n",
    "     while True:\n",
    "\n",
    "            # Show environment (the visual) [WIP]\n",
    "        env.render()\n",
    "\n",
    "            # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "            # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "            # Learn\n",
    "        q, loss = mario.learn()\n",
    "        \n",
    "            # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "            # Update state\n",
    "        state = next_state\n",
    "\n",
    "            # Check if end of game\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "        \n",
    "     logger.log_episode()\n",
    "\n",
    "     if e % 20 == 0:\n",
    "        mario.save()\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "                step=mario.curr_step\n",
    "            )\n",
    "         \n",
    "#env.close_video_recorder()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50435b4-1b92-465d-820b-3ee45d8c876d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a383d-bf50-47dc-94c5-017f7c76cb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
